# -*- coding: utf-8 -*-
"""TinyML.101.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Gx5NVHlv4vJsNUGtCAdEHuqzN2mXY-x9

## TinyML, 101
"""

# Install TensorFLow
# !pip install tensorflow==2.2.0
#
import tensorflow as tf
#
import numpy as np
#
import matplotlib.pyplot as plt
#
import math
#

print("TensorFlow version", tf.__version__)

"""## Generating sample data"""

# Generate sample datapoints
SAMPLES = 1000
SEED = 1337

np.random.seed(SEED)
tf.random.set_seed(SEED)

# Generate a uniformly distributed set of random numbers in the range from
# 0 to 2n, wich covers a complete sine wave oscillation
x_values = np.random.uniform(low=0, high=2*math.pi, size=SAMPLES)

# Shuffle the values to garantee they're not in order
np.random.shuffle(x_values)

# Calculate the correspding sine values
y_values = np.sin(x_values)

# Plot the data, in blue.
plt.plot(x_values, y_values, 'b.')
plt.show()

# Add small randomness...
y_values += 0.1 * np.random.randn(*y_values.shape)

# Re-plot (4-10)
plt.plot(x_values, y_values, 'r.')
plt.show()

TRAIN_SPLIT = int(0.6 * SAMPLES)
TEST_SPLIT = int((0.2 * SAMPLES) + TRAIN_SPLIT)

print(TEST_SPLIT)

x_train, x_validate, x_test = np.split(x_values, [TRAIN_SPLIT, TEST_SPLIT])
y_train, y_validate, y_test = np.split(y_values, [TRAIN_SPLIT, TEST_SPLIT])

assert (x_train.size + x_validate.size + x_test.size) == SAMPLES

"""Plot each data in a different color"""

plt.plot(x_train, y_train, 'b.', label="Train")
plt.plot(x_validate, y_validate, 'y.', label="Validate")
plt.plot(x_test, y_test, 'r.', label="Test")
plt.legend()
plt.show()

"""### Now defining a Basic Model"""

# Using Keras
# import tensorflow as tf
print("Reminder TF version:", tf.__version__)
#
from tensorflow.keras import layers
# from tensorflow import keras
print("Keras version:", tf.keras.__version__)
# from keras import layers
model_1 = tf.keras.models.Sequential()
#

"""##### First layer takes a scalar input and feeds it into 16 neurons.
Activation is based on **relu** activation function.
"""

model_1.add(layers.Dense(16, activation='relu', input_shape=(1,)))

"""Final layer is a single neuron using a standard optimizer and loss function for regression"""

model_1.add(layers.Dense(1))

# Compile
model_1.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])
# Print model's architecture
model_1.summary()
#

